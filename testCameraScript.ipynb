{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ef96ae5-84d6-45a4-8b42-63c86ddc1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ 2024-11-1 Python-3.6.9 torch-1.10.1+cu102 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Python 3.7.0 is required by YOLO, but Python 3.6.9 is currently installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "yolov9-c summary: 604 layers, 50698278 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://192.168.95.177:5000/ (Press CTRL+C to quit)\n",
      "192.168.96.99 - - [01/Nov/2024 15:20:11] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.96.99 - - [01/Nov/2024 15:20:14] \"GET /video_feed HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from flask import Flask, Response, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Path to the YOLOv9 file\n",
    "yolo_weights_path = '/home/nvinhhung/yolo/yolo9_face/yolov9-face-detection/yolov9/best.pt'\n",
    "\n",
    "# Check for the existence of the YOLOv9 file\n",
    "if not os.path.exists(yolo_weights_path):\n",
    "    print(\"Error: YOLO weights file path does not exist.\")\n",
    "\n",
    "# Load the YOLOv9 model with PyTorch\n",
    "yolo_model = torch.hub.load('/home/nvinhhung/yolo/yolo9_face/yolov9-face-detection/yolov9', 'custom', path=yolo_weights_path, source='local')\n",
    "\n",
    "# Load the trained model for emotions\n",
    "model_path = '/home/nvinhhung/best_model_2.keras'\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Error: Model file path does not exist.\")\n",
    "\n",
    "emotion_model = load_model(model_path)\n",
    "emotion_model.compile(optimizer=Adamax(learning_rate=0.001),\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy', Precision(), Recall()])\n",
    "\n",
    "# List of Emotion Labels\n",
    "emotion_labels = ['anger', 'disgust', 'fear', 'happy', 'pain', 'sad']\n",
    "\n",
    "# Create a video stream and perform emotion prediction\n",
    "def generate_frames():\n",
    "    cap = cv2.VideoCapture(0)  # Use the default camera\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video capture.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print(\"Error: Could not read frame from video capture.\")\n",
    "                break\n",
    "\n",
    "            # Convert the frame to a tensor\n",
    "            frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float()  # Tá»« HWC sang CHW\n",
    "            frame_tensor /= 255.0  # Chuáº©n hÃ³a giÃ¡ trá»‹ pixel\n",
    "            results = yolo_model(frame_tensor.unsqueeze(0))  # ThÃªm chiá»u batch\n",
    "\n",
    "            # Check and process detection results\n",
    "            if isinstance(results, list) and len(results) > 0:\n",
    "                detections = results[0]  # Láº¥y Ä‘á»‘i tÆ°á»£ng Ä‘áº§u tiÃªn\n",
    "                detections = detections.cpu().numpy() if torch.is_tensor(detections) else detections\n",
    "            else:\n",
    "                detections = np.empty((0, 6))  # If no results are found, return an empty array\n",
    "\n",
    "            # Filter detection for person (class_id = 0 for person in the COCO dataset)\n",
    "            persons = []\n",
    "            for detection in detections:\n",
    "                if len(detection) < 6:  # Ensure that the detection has enough information\n",
    "                    continue\n",
    "                *box, conf, class_id = detection\n",
    "                if int(class_id) == 0 and conf > 0.5:  # \"class_id 0 is 'person' in COCO.\"\n",
    "                    x, y, x2, y2 = map(int, box)\n",
    "                    w, h = x2 - x, y2 - y\n",
    "                    persons.append((x, y, w, h))\n",
    "\n",
    "            # Predict emotions for each detected person\n",
    "            for (x, y, w, h) in persons:\n",
    "                person_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Convert to RGB and resize for the emotion model\n",
    "                face = cv2.resize(person_roi, (299, 299))  # Ensure the input size is correct.\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "                face = img_to_array(face) / 255.0  # standardization\n",
    "                face = np.expand_dims(face, axis=0)\n",
    "\n",
    "                # Emotion prediction\n",
    "                prediction = emotion_model.predict(face)\n",
    "                emotion_index = np.argmax(prediction)\n",
    "                emotion = emotion_labels[emotion_index]\n",
    "\n",
    "                # Draw emotional labels on the face.\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "            # Convert the frame to JPEG\n",
    "            ret, buffer = cv2.imencode('.jpg', frame)\n",
    "            if not ret:\n",
    "                print(\"Error: Could not encode frame to JPEG.\")\n",
    "                break\n",
    "\n",
    "            frame = buffer.tobytes()\n",
    "            yield (b'--frame\\r\\n'\n",
    "                   b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n')\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aae0c4-5618-4c96-8667-ecc1b01c44b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
